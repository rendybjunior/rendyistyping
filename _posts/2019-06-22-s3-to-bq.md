---
title: "Simple Way to Load Date-Partitioned Parquet Files on S3 to BigQuery using Cloud Composer"
date: 2019-06-22
category: Data Engineering
tags: [airflow, composer, gcp, bigquery, gcs, s3]
---

In this post, I would like to elaborate how I load my date-partitioned Parquet files on S3 to BigQuery using Airflow. I will also mention some technical issues I met along the way.

## Aiflow to easily manage load tasks
As I am loading tens of tables for several years of data, using Airflow is a must. I have experienced backfilling using ad hoc script, and it was super hard and tedious to check which date the job failed, fix the issue, and rerun failed dates. Using Airflow is the easiest way to check if my backfill is failed on certain date as I can easily view it on Airflow Dashboard, specifically  DAG's Tree View. Failed tasks will have red color. In this case I am using managed Airflow, Cloud Composer, on Google Cloud Platform.

<figure class="third">
	<img src="/assets/images/dag_tree_view.png">
  <figcaption>DAG Tree View</figcaption>
</figure>

## Cost-effective way to transfer S3 files to GCS
There are some alternatives to copy S3 files to GCS.
- Create a machine to download data from S3 and load to GCS
- Use big data processing such as Spark to read S3 and load to GCS
- Use Storage Transfer Service

The cheapest and easiest way to copy file as is is to use the third method, using Storage Transfer Service. It is cheaper because it is free! You don't have to spawn any machines, so you will only incur AWS Out Data Transfer or egress. No cost on GCP beside storage cost. Based on several tries, it is quite fast and also very simple to use.

## How the steps are tied together
Here is the overall flow:
> `S3 --(Storage Transfer Service)--> GCS --(BQ Load)--> BigQuery`

S3 Parquet files will be copied first to GCS. After it is copied completely to GCS, then the files are loaded to BigQuery. I am using `S3ToGoogleCloudStorageTransferOperator` operator to spawn Storage Transfer Service job. The task will keep waiting until the job finishes. After data loaded to GCS, I am using `GoogleCloudStorageToBigQueryOperator` operator to load data to BigQuery. The task create a BigQuery load job with specified parameters.

<figure class="third">
	<img src="/assets/images/s3_to_bq_dag.png">
  <figcaption>DAG S3 to BigQuery</figcaption>
</figure>

## Handling Date-Partitioned Files
To handle date-partitioned files, I am using airflow's template variable such as ds.
```
include_prefix = 'my_data/parquet/day_1/my_event/event_date={{{{ ds }}}}/part-' # data partitioned by date
destination_table = 'my-gcp-project.mydataset.my_event${{{{ ds_nodash }}}}' # bq partitioned by date
```
Variables above are being used to specify which S3 files to copy and to which BigQuery table partition to load. Here is how the variable is being used in S3 to GCS operator.
```
s3_to_gcs = S3ToGoogleCloudStorageTransferOperator(
    ...
    object_conditions={ 'include_prefixes': [ include_prefix ] },
    ...
)
```
And here is how the variable is being used in GCS to BigQuery load operator.
```
gcs_to_bq = GoogleCloudStorageToBigQueryOperator(
    ...
    source_objects=[ include_prefix + '*' ],
    destination_project_dataset_table=destination_table,
    ...
)
```

## Full code example
You can find example of the dag here https://github.com/rendybjunior/cloud-composer-examples/blob/master/dags/s3_to_bq_dag_example.py
